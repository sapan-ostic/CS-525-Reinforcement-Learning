{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sapanostic/anaconda3/envs/rl/lib/python3.7/site-packages/chainer/backends/cuda.py:143: UserWarning: cuDNN is not enabled.\n",
      "Please reinstall CuPy after you install cudnn\n",
      "(see https://docs-cupy.chainer.org/en/stable/install.html#install-cudnn).\n",
      "  'cuDNN is not enabled.\\n'\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import minerl\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "from logging import getLogger\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "# from env_wrappers import (SerialDiscreteActionWrapper, FrameSkip, PoVWithCompassAngleWrapper)\n",
    "from chainerrl.wrappers.atari_wrappers import LazyFrames\n",
    "import numpy as np\n",
    "                          \n",
    "logger = getLogger(__name__)\n",
    "# import logging\n",
    "\n",
    "# Download Single experiment\n",
    "# minerl.data.download('/home/sapanostic/Courses/WPI_RL/Project/minerl_data/',experiment='MineRLObtainDiamond-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below line to see the logging process\n",
    "# logging.basicConfig(level=logging.DEBUG)\n",
    "env = gym.make('MineRLNavigate-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "The obs variable will be a dictionary containing the following observations returned by the environment. In the case of the MineRLNavigate-v0 environment, three observations are returned:\n",
    "\n",
    "1) pov: an RGB image of the agent’s first person perspective (64,64,3)\n",
    "\n",
    "2) compassAngle, a float giving the angle of the agent to its (approximate) target\n",
    "\n",
    "3) inventory, a dictionary containing the amount of 'dirt' blocks in the agent’s inventory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict(attack:Discrete(2), back:Discrete(2), camera:Box(2,), forward:Discrete(2), jump:Discrete(2), left:Discrete(2), place:Enum(none,dirt), right:Discrete(2), sneak:Discrete(2), sprint:Discrete(2))\n",
      "number of objects in Observation Dict:  3\n",
      "First person view (pov) image size:  (64, 64, 3)\n",
      "Inventory:  {'dirt': 0}\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(\"number of objects in Observation Dict: \", len(obs))\n",
    "print(\"First person view (pov) image size: \", np.shape(obs['pov']))\n",
    "print(\"Inventory: \", obs['inventory'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(64, 64, 3)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.spaces['pov'])\n",
    "a_ = env.observation_space.spaces['pov']\n",
    "print(a_.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k, channel_order='hwc', use_tuple=False):\n",
    "        \"\"\"Stack k last frames.\n",
    "\n",
    "        Returns lazy array, which is much more memory efficient.\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.k = k\n",
    "        self.observations = deque([], maxlen=k)\n",
    "        self.stack_axis = {'hwc': 2, 'chw': 0}[channel_order]\n",
    "        self.use_tuple = use_tuple\n",
    "\n",
    "        if self.use_tuple:\n",
    "            pov_space = env.observation_space[0]\n",
    "            inv_space = env.observation_space[1]\n",
    "        else:\n",
    "            pov_space = env.observation_space\n",
    "\n",
    "        low_pov = np.repeat(pov_space.low, k, axis=self.stack_axis)\n",
    "        high_pov = np.repeat(pov_space.high, k, axis=self.stack_axis)\n",
    "        pov_space = gym.spaces.Box(low=low_pov, high=high_pov, dtype=pov_space.dtype)\n",
    "\n",
    "        if self.use_tuple:\n",
    "            low_inv = np.repeat(inv_space.low, k, axis=0)\n",
    "            high_inv = np.repeat(inv_space.high, k, axis=0)\n",
    "            inv_space = gym.spaces.Box(low=low_inv, high=high_inv, dtype=inv_space.dtype)\n",
    "            self.observation_space = gym.spaces.Tuple(\n",
    "                (pov_space, inv_space))\n",
    "        else:\n",
    "            self.observation_space = pov_space\n",
    "\n",
    "    def reset(self):\n",
    "        ob = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.observations.append(ob)\n",
    "        return self._get_ob()\n",
    "\n",
    "    def step(self, action):\n",
    "        ob, reward, done, info = self.env.step(action)\n",
    "        self.observations.append(ob)\n",
    "        return self._get_ob(), reward, done, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        assert len(self.observations) == self.k\n",
    "        if self.use_tuple:\n",
    "            frames = [x[0] for x in self.observations]\n",
    "            inventory = [x[1] for x in self.observations]\n",
    "            return (LazyFrames(list(frames), stack_axis=self.stack_axis),\n",
    "                    LazyFrames(list(inventory), stack_axis=0))\n",
    "        else:\n",
    "            return LazyFrames(list(self.observations), stack_axis=self.stack_axis)\n",
    "\n",
    "class FrameSkip(gym.Wrapper):\n",
    "    \"\"\"Return every `skip`-th frame and repeat given action during skip.\n",
    "\n",
    "    Note that this wrapper does not \"maximize\" over the skipped frames.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class PoVWithCompassAngleWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"Take 'pov' value (current game display) and concatenate compass angle information with it, as a new channel of image;\n",
    "    resulting image has RGB+compass (or K+compass for gray-scaled image) channels.\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self._compass_angle_scale = 180 / 255  # NOTE: `ScaledFloatFrame` will scale the pixel values with 255.0 later\n",
    "\n",
    "        pov_space = self.env.observation_space.spaces['pov']\n",
    "        compass_angle_space = self.env.observation_space.spaces['compassAngle']\n",
    "\n",
    "        low = self.observation({'pov': pov_space.low, 'compassAngle': compass_angle_space.low})\n",
    "        high = self.observation({'pov': pov_space.high, 'compassAngle': compass_angle_space.high})\n",
    "\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        pov = observation['pov']\n",
    "        compass_scaled = observation['compassAngle'] / self._compass_angle_scale\n",
    "        compass_channel = np.ones(shape=list(pov.shape[:-1]) + [1], dtype=pov.dtype) * compass_scaled\n",
    "        return np.concatenate([pov, compass_channel], axis=-1)\n",
    "\n",
    "class GrayScaleWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, dict_space_key=None):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self._key = dict_space_key\n",
    "\n",
    "        if self._key is None:\n",
    "            original_space = self.observation_space\n",
    "        else:\n",
    "            original_space = self.observation_space.spaces[self._key]\n",
    "        height, width = original_space.shape[0], original_space.shape[1]\n",
    "\n",
    "        # sanity checks\n",
    "        ideal_image_space = gym.spaces.Box(low=0, high=255, shape=(height, width, 3), dtype=np.uint8)\n",
    "        if original_space != ideal_image_space:\n",
    "            raise ValueError('Image space should be {}, but given {}.'.format(ideal_image_space, original_space))\n",
    "        if original_space.dtype != np.uint8:\n",
    "            raise ValueError('Image should `np.uint8` typed, but given {}.'.format(original_space.dtype))\n",
    "\n",
    "        height, width = original_space.shape[0], original_space.shape[1]\n",
    "        new_space = gym.spaces.Box(low=0, high=255, shape=(height, width, 1), dtype=np.uint8)\n",
    "        if self._key is None:\n",
    "            self.observation_space = new_space\n",
    "        else:\n",
    "            new_space_dict = copy.deepcopy(self.observation_space)\n",
    "            new_space_dict.spaces[self._key] = new_space\n",
    "            self.observation_space = new_space_dict\n",
    "\n",
    "    def observation(self, obs):\n",
    "        if self._key is None:\n",
    "            frame = obs\n",
    "        else:\n",
    "            frame = obs[self._key]\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = np.expand_dims(frame, -1)\n",
    "        if self._key is None:\n",
    "            obs = frame\n",
    "        else:\n",
    "            obs[self._key] = frame\n",
    "        return obs\n",
    "    \n",
    "class SerialDiscreteActionWrapper(gym.ActionWrapper):\n",
    "    \"\"\"Convert MineRL env's `Dict` action space as a serial discrete action space.\n",
    "\n",
    "    The term \"serial\" means that this wrapper can only push one key at each step.\n",
    "    \"attack\" action will be alwarys triggered.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env\n",
    "        Wrapping gym environment.\n",
    "    always_keys\n",
    "        List of action keys, which should be always pressed throughout interaction with environment.\n",
    "        If specified, the \"noop\" action is also affected.\n",
    "    reverse_keys\n",
    "        List of action keys, which should be always pressed but can be turn off via action.\n",
    "        If specified, the \"noop\" action is also affected.\n",
    "    exclude_keys\n",
    "        List of action keys, which should be ignored for discretizing action space.\n",
    "    exclude_noop\n",
    "        The \"noop\" will be excluded from discrete action list.\n",
    "    num_camera_discretize\n",
    "        Number of discretization of yaw control (must be odd).\n",
    "    allow_pitch\n",
    "        If specified, this wrapper appends commands to control pitch.\n",
    "    max_camera_range\n",
    "        Maximum value of yaw control.\n",
    "    \"\"\"\n",
    "\n",
    "    BINARY_KEYS = ['forward', 'back', 'left', 'right', 'jump', 'sneak', 'sprint', 'attack']\n",
    "\n",
    "    def __init__(self, env, always_keys=None, reverse_keys=None, exclude_keys=None, exclude_noop=False,\n",
    "                 num_camera_discretize=3, allow_pitch=False,\n",
    "                 max_camera_range=10):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.always_keys = [] if always_keys is None else always_keys\n",
    "        self.reverse_keys = [] if reverse_keys is None else reverse_keys\n",
    "        self.exclude_keys = [] if exclude_keys is None else exclude_keys\n",
    "        if len(set(self.always_keys) | set(self.reverse_keys) | set(self.exclude_keys)) != \\\n",
    "                len(self.always_keys) + len(self.reverse_keys) + len(self.exclude_keys):\n",
    "            raise ValueError('always_keys ({}) or reverse_keys ({}) or exclude_keys ({}) intersect each other.'.format(\n",
    "                self.always_keys, self.reverse_keys, self.exclude_keys))\n",
    "        self.exclude_noop = exclude_noop\n",
    "\n",
    "        self.wrapping_action_space = self.env.action_space\n",
    "        self.num_camera_discretize = num_camera_discretize\n",
    "        self._noop_template = OrderedDict([\n",
    "            ('forward', 0),\n",
    "            ('back', 0),\n",
    "            ('left', 0),\n",
    "            ('right', 0),\n",
    "            ('jump', 0),\n",
    "            ('sneak', 0),\n",
    "            ('sprint', 0),\n",
    "            ('attack', 0),\n",
    "            ('camera', np.zeros((2, ), dtype=np.float32)),\n",
    "            # 'none', 'dirt' (Obtain*:)+ 'stone', 'cobblestone', 'crafting_table', 'furnace', 'torch'\n",
    "            ('place', 0),\n",
    "            # (Obtain* tasks only) 'none', 'wooden_axe', 'wooden_pickaxe', 'stone_axe', 'stone_pickaxe', 'iron_axe', 'iron_pickaxe'\n",
    "            ('equip', 0),\n",
    "            # (Obtain* tasks only) 'none', 'torch', 'stick', 'planks', 'crafting_table'\n",
    "            ('craft', 0),\n",
    "            # (Obtain* tasks only) 'none', 'wooden_axe', 'wooden_pickaxe', 'stone_axe', 'stone_pickaxe', 'iron_axe', 'iron_pickaxe', 'furnace'\n",
    "            ('nearbyCraft', 0),\n",
    "            # (Obtain* tasks only) 'none', 'iron_ingot', 'coal'\n",
    "            ('nearbySmelt', 0),\n",
    "        ])\n",
    "        for key, space in self.wrapping_action_space.spaces.items():\n",
    "            if key not in self._noop_template:\n",
    "                raise ValueError('Unknown action name: {}'.format(key))\n",
    "\n",
    "        # get noop\n",
    "        self.noop = copy.deepcopy(self._noop_template)\n",
    "        for key in self._noop_template:\n",
    "            if key not in self.wrapping_action_space.spaces:\n",
    "                del self.noop[key]\n",
    "\n",
    "        # check&set always_keys\n",
    "        for key in self.always_keys:\n",
    "            if key not in self.BINARY_KEYS:\n",
    "                raise ValueError('{} is not allowed for `always_keys`.'.format(key))\n",
    "            self.noop[key] = 1\n",
    "        logger.info('always pressing keys: {}'.format(self.always_keys))\n",
    "        # check&set reverse_keys\n",
    "        for key in self.reverse_keys:\n",
    "            if key not in self.BINARY_KEYS:\n",
    "                raise ValueError('{} is not allowed for `reverse_keys`.'.format(key))\n",
    "            self.noop[key] = 1\n",
    "        logger.info('reversed pressing keys: {}'.format(self.reverse_keys))\n",
    "        # check exclude_keys\n",
    "        for key in self.exclude_keys:\n",
    "            if key not in self.noop:\n",
    "                raise ValueError('unknown exclude_keys: {}'.format(key))\n",
    "        logger.info('always ignored keys: {}'.format(self.exclude_keys))\n",
    "\n",
    "        # get each discrete action\n",
    "        self._actions = [self.noop]\n",
    "        for key in self.noop:\n",
    "            if key in self.always_keys or key in self.exclude_keys:\n",
    "                continue\n",
    "            if key in self.BINARY_KEYS:\n",
    "                # action candidate : {1}  (0 is ignored because it is for noop), or {0} when `reverse_keys`.\n",
    "                op = copy.deepcopy(self.noop)\n",
    "                if key in self.reverse_keys:\n",
    "                    op[key] = 0\n",
    "                else:\n",
    "                    op[key] = 1\n",
    "                self._actions.append(op)\n",
    "            elif key == 'camera':\n",
    "                # action candidate : {[0, -max_camera_range], [0, -max_camera_range + delta_range], ..., [0, max_camera_range]}\n",
    "                # ([0, 0] is excluded)\n",
    "                delta_range = max_camera_range * 2 / (self.num_camera_discretize - 1)\n",
    "                if self.num_camera_discretize % 2 == 0:\n",
    "                    raise ValueError('Number of camera discretization must be odd.')\n",
    "             \n",
    "                for i in range(self.num_camera_discretize):\n",
    "                    op = copy.deepcopy(self.noop)\n",
    "                    if i < self.num_camera_discretize // 2:\n",
    "                        op[key] = np.array([0, -max_camera_range + delta_range * i], dtype=np.float32)\n",
    "                    elif i > self.num_camera_discretize // 2:\n",
    "                        op[key] = np.array([0, -max_camera_range + delta_range * (i - 1)], dtype=np.float32)\n",
    "                    elif i == self.num_camera_discretize // 2:\n",
    "                        op[key] = np.array([0, -max_camera_range + delta_range * (i + 1)], dtype=np.float32)                            \n",
    "                    else:\n",
    "                        continue\n",
    "                    self._actions.append(op)\n",
    "\n",
    "                if allow_pitch:\n",
    "                    for i in range(self.num_camera_discretize):\n",
    "                        op = copy.deepcopy(self.noop)\n",
    "                        if i < self.num_camera_discretize // 2:\n",
    "                            op[key] = np.array([-max_camera_range + delta_range * i, 0], dtype=np.float32)\n",
    "                        elif i > self.num_camera_discretize // 2:\n",
    "                            op[key] = np.array([-max_camera_range + delta_range * (i - 1), 0], dtype=np.float32)\n",
    "                        else:\n",
    "                            continue\n",
    "                        self._actions.append(op)\n",
    "\n",
    "            elif key in {'place', 'equip', 'craft', 'nearbyCraft', 'nearbySmelt'}:\n",
    "                # action candidate : {1, 2, ..., len(space)-1}  (0 is ignored because it is for noop)\n",
    "                for a in range(1, self.wrapping_action_space.spaces[key].n):\n",
    "                    op = copy.deepcopy(self.noop)\n",
    "                    op[key] = a\n",
    "                    self._actions.append(op)\n",
    "        if self.exclude_noop:\n",
    "            del self._actions[0]\n",
    "\n",
    "        n = len(self._actions)\n",
    "        self.action_space = gym.spaces.Discrete(n)\n",
    "        logger.info('{} is converted to {}.'.format(self.wrapping_action_space, self.action_space))\n",
    "\n",
    "    def action(self, action):\n",
    "        if not self.action_space.contains(action):\n",
    "            raise ValueError('action {} is invalid for {}'.format(action, self.action_space))\n",
    "\n",
    "        original_space_action = self._actions[action]\n",
    "        logger.debug('discrete action {} -> original action {}'.format(action, original_space_action))\n",
    "        return original_space_action\n",
    "    \n",
    "    def sample(self):\n",
    "        rand_action = np.random.choice(np.array(range(self.action_space.n)),1)\n",
    "        return rand_action[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:  7\n",
      "observation_space:  (64, 64, 8)\n",
      "OrderedDict([('forward', 1), ('back', 0), ('left', 1), ('right', 0), ('jump', 1), ('sneak', 0), ('sprint', 0), ('attack', 1), ('camera', array([0., 0.], dtype=float32)), ('place', 0)])\n"
     ]
    }
   ],
   "source": [
    "always_keys = ['forward', 'attack', 'jump']\n",
    "exclude_keys = ['back', 'place', 'sneak']\n",
    "reverse_keys=None\n",
    "exclude_noop=False\n",
    "num_camera_discretize=3\n",
    "allow_pitch=False\n",
    "max_camera_range=10\n",
    "\n",
    "\n",
    "# env_Gray = GrayScaleWrapper(env, dict_space_key='pov')\n",
    "\n",
    "env_FSkip= FrameSkip(env)\n",
    "env_Gray = GrayScaleWrapper(env_FSkip, dict_space_key='pov')\n",
    "env_pov_comm = PoVWithCompassAngleWrapper(env_Gray)\n",
    "env_FStack = FrameStack(env_pov_comm, 4)\n",
    "\n",
    "env_serial = SerialDiscreteActionWrapper(env_FStack, always_keys, reverse_keys, exclude_keys, exclude_noop,\n",
    "                 num_camera_discretize, allow_pitch,\n",
    "                 max_camera_range)\n",
    "\n",
    "print('Action Space: ', env_serial.action_space.n)\n",
    "print('observation_space: ', env_serial.observation_space.shape)\n",
    "\n",
    "# print(env_serial.noop)\n",
    "print(env_serial.action(1))\n",
    "\n",
    "#0:Noop \n",
    "#1:left \n",
    "#2:right \n",
    "#3:sprint \n",
    "#4:Camera[0,-10] \n",
    "#5:Camera[0,10]\n",
    "#6:Camera[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "obs_ = np.asarray(obs[:,:,:3]/255, dtype=np.float32)\n",
    "# print(np.shape(obs_[:,:,:3]))\n",
    "# print(obs_[:,:,0:3])\n",
    "plt.imshow(obs_[:,:,0:3])\n",
    "y = cv2.cvtColor(obs_[:,:,:3], cv2.COLOR_RGB2GRAY)\n",
    "print(y)\n",
    "np.concatenate((y, obs[:,:,3]), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "4\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "4\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "4\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "1\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "4\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "1\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "1\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "4\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "1\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "4\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "1\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "1\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "4\n",
      "Total reward:  0.0\n",
      "1\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "4\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "1\n",
      "Total reward:  0.0\n",
      "1\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "1\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "2\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "1\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "3\n",
      "Total reward:  0.0\n",
      "4\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n",
      "6\n",
      "Total reward:  0.0\n",
      "4\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "5\n",
      "Total reward:  0.0\n",
      "0\n",
      "Total reward:  0.0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "net_reward = 0\n",
    "obs_ = env_serial.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    a_ = env_serial.sample()\n",
    "    print(a_)\n",
    "#     action_ = env_serial.action(a_)\n",
    "#     print(action_)\n",
    "    obs, reward, done, info = env_serial.step(a_)\n",
    "    net_reward += reward\n",
    "    print(\"Total reward: \", net_reward)\n",
    "    \n",
    "# env_serial.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(info['life'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 64, 64, 2)\n",
      "(4, 2, 64, 64)\n",
      "(8, 64, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[157., 157., 157., ...,  70.,  70., 101.],\n",
       "       [157., 157., 157., ...,  70., 101., 102.],\n",
       "       [157., 157., 157., ..., 101., 102., 102.],\n",
       "       ...,\n",
       "       [157., 157., 157., ...,  99., 100.,  98.],\n",
       "       [157., 157., 157., ...,  96.,  99.,  99.],\n",
       "       [157., 157., 156., ..., 101.,  96.,  99.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.array(obs_._frames).shape)\n",
    "s_ = np.array(obs_._frames)\n",
    "c = s_.swapaxes(1,3)\n",
    "print(c.shape)\n",
    "d = c.reshape(8,64,64)\n",
    "print(d.shape)\n",
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[157.        , 157.        , 157.        , 157.        ],\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         ...,\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         [157.        , 157.        , 157.        , 157.        ]],\n",
       "\n",
       "        [[157.        , 157.        , 157.        , 157.        ],\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         ...,\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         [157.        , 157.        , 157.        , 157.        ]],\n",
       "\n",
       "        [[157.        , 157.        , 157.        , 157.        ],\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         ...,\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         [157.        , 157.        , 157.        , 157.        ],\n",
       "         [156.        , 156.        , 156.        , 156.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 70.        ,  70.        ,  70.        ,  70.        ],\n",
       "         [ 70.        ,  70.        ,  70.        ,  70.        ],\n",
       "         [101.        , 101.        , 101.        , 101.        ],\n",
       "         ...,\n",
       "         [ 99.        ,  99.        ,  99.        ,  99.        ],\n",
       "         [ 96.        ,  96.        ,  96.        ,  96.        ],\n",
       "         [101.        , 101.        , 101.        , 101.        ]],\n",
       "\n",
       "        [[ 70.        ,  70.        ,  70.        ,  70.        ],\n",
       "         [101.        , 101.        , 101.        , 101.        ],\n",
       "         [102.        , 102.        , 102.        , 102.        ],\n",
       "         ...,\n",
       "         [100.        , 100.        , 100.        , 100.        ],\n",
       "         [ 99.        ,  99.        ,  99.        ,  99.        ],\n",
       "         [ 96.        ,  96.        ,  96.        ,  96.        ]],\n",
       "\n",
       "        [[101.        , 101.        , 101.        , 101.        ],\n",
       "         [102.        , 102.        , 102.        , 102.        ],\n",
       "         [102.        , 102.        , 102.        , 102.        ],\n",
       "         ...,\n",
       "         [ 98.        ,  98.        ,  98.        ,  98.        ],\n",
       "         [ 99.        ,  99.        ,  99.        ,  99.        ],\n",
       "         [ 99.        ,  99.        ,  99.        ,  99.        ]]],\n",
       "\n",
       "\n",
       "       [[[-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         ...,\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875]],\n",
       "\n",
       "        [[-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         ...,\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875]],\n",
       "\n",
       "        [[-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         ...,\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         ...,\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875]],\n",
       "\n",
       "        [[-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         ...,\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875]],\n",
       "\n",
       "        [[-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         ...,\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875],\n",
       "         [-86.90822875, -86.90822875, -86.90822875, -86.90822875]]]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_rl_37",
   "language": "python",
   "name": "conda_rl_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
