{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Liam's DQN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "NHQ8dw3q91LN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download Pretrained Model\n",
        "This model is only trained to play Pong"
      ]
    },
    {
      "metadata": {
        "id": "9KSRJSWT96VY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/LiamHz/PretrainedModels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6a0xunL8GJ_X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab Installs + Regular Imports\n",
        "If you're running this script outside of a notebook set Colab to 'False'\n",
        "gsync allows this notebook to save pretrained models directly to your Google Drive account"
      ]
    },
    {
      "metadata": {
        "id": "_Amw1q8s_2vt",
        "colab_type": "code",
        "outputId": "59e8239b-20b7-406d-86fe-6efc74feb2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "# Google Colab PyTorch utilities\n",
        "!wget https://raw.githubusercontent.com/StefOe/colab-pytorch-utils/master/utils.py\n",
        "import utils\n",
        "gsync = utils.GDriveSync()\n",
        "\n",
        "! pip install torch\n",
        "! pip install gym\n",
        "! pip install opencv-python\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import cv2\n",
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "COLAB = True\n",
        "\n",
        "if not COLAB:\n",
        "    from lib import wrappers\n",
        "    from lib import dqn_model\n",
        "\n",
        "    import argparse\n",
        "    from tensorboardX import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-30 22:55:17--  https://raw.githubusercontent.com/StefOe/colab-pytorch-utils/master/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4519 (4.4K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]   4.41K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-01-30 22:55:17 (74.0 MB/s) - ‘utils.py’ saved [4519/4519]\n",
            "\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (3.4.5.20)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.14.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fpp316gJGXZp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# OpenAI Gym Wrappers\n",
        "These wrappers make it easier to interact with OpenAI Gym\n",
        "\n",
        "Wrappers include:\n",
        "\n",
        "\n",
        "*   Frame skipping\n",
        "*   Frame processing (downsampling and greyscaling)\n",
        "* Image normalization and converting to PyTorch\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "zTZyTsre_3lG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Taken from OpenAI baseline wrappers\n",
        "# https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear past frame buffer and init to first obs\"\"\"\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Downsamples image to 84x84\n",
        "    Greyscales image\n",
        "\n",
        "    Returns numpy array\n",
        "    \"\"\"\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
        "                                                dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaledFloatFrame(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MyeLIpznGhBu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DQN Architecture\n",
        "Deep-Q-Networks (DQNs) are composed of: \n",
        "* 3 convolution layers\n",
        "* 2 fully-connected linear layers"
      ]
    },
    {
      "metadata": {
        "id": "uyvSSjc1GjIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FvdTslVUGmdH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Parameters\n",
        "Parameters\n",
        "* REPLAY_SIZE: Maximum number of experiences stored in replay memory\n",
        "* TARGET_UPDATE_FREQ: How many frames in between syncing target DQN with behaviour DQN\n",
        "* LEARNING_STARTS: Number of experiences to add to replay memory before training network"
      ]
    },
    {
      "metadata": {
        "id": "Tcm5N8KP0sF-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19.5\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10 ** 4 * 4\n",
        "LEARNING_RATE = 1e-4\n",
        "TARGET_UPDATE_FREQ = 1000\n",
        "LEARNING_STARTS = 50000\n",
        "\n",
        "EPSILON_DECAY = 10**5\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.02\n",
        "\n",
        "MODEL = \"PretrainedModels/PongNoFrameskip-v4-407.dat\"\n",
        "LOAD_MODEL = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I9fVaD2M0wLc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experience Replay"
      ]
    },
    {
      "metadata": {
        "id": "9KVzom9K0zIf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "\n",
        "class ExperienceReplay:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oyc4Y9bB01t0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ]
    },
    {
      "metadata": {
        "id": "iCPK9nrO05Mk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, replay_memory):\n",
        "        self.env = env\n",
        "        self.replay_memory = replay_memory\n",
        "        self._reset()\n",
        "        self.last_action = 0\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "        \"\"\"\n",
        "        Select action\n",
        "        Execute action and step environment\n",
        "        Add state/action/reward to experience replay\n",
        "        \"\"\"\n",
        "        done_reward = None\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "        new_state = new_state\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.replay_memory.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dKt-mco0910",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loss function "
      ]
    },
    {
      "metadata": {
        "id": "7ARCnK-B1C3W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calculate_loss(batch, net, target_net, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Calculate MSE between actual state action values,\n",
        "    and expected state action values from DQN\n",
        "    \"\"\"\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    states_v = torch.tensor(states).to(device)\n",
        "    next_states_v = torch.tensor(next_states).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "    state_action_values = net(states_v).gather(1, actions_v.long().unsqueeze(-1)).squeeze(-1)\n",
        "    next_state_values = target_net(next_states_v).max(1)[0]\n",
        "    next_state_values[done] = 0.0\n",
        "    next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zwYEFJAn1FOK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ]
    },
    {
      "metadata": {
        "id": "GDZ6Kub4Gopl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"ReplayMemory will require {}gb of GPU RAM\".format(round(REPLAY_SIZE * 32 * 84 * 84 / 1e+9, 2)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if COLAB:\n",
        "        \"\"\"Default argparse does not work on colab\"\"\"\n",
        "        class ColabArgParse():\n",
        "            def __init__(self, cuda, env, reward, model):\n",
        "                self.cuda = cuda\n",
        "                self.env = env\n",
        "                self.reward = reward\n",
        "                self.model = model\n",
        "\n",
        "        args = ColabArgParse(CUDA, ENV_NAME, MEAN_REWARD_BOUND, MODEL)\n",
        "    else:\n",
        "        parser = argparse.ArgumentParser()\n",
        "        parser.add_argument(\"--cuda\", default=True, action=\"store_true\", help=\"Enable cuda\")\n",
        "        parser.add_argument(\"--env\", default=ENV_NAME,\n",
        "                            help=\"Name of the environment, default=\" + ENV_NAME)\n",
        "        parser.add_argument(\"--reward\", type=float, default=MEAN_REWARD_BOUND,\n",
        "                            help=\"Mean reward to stop training, default={}\".format(round(MEAN_REWARD_BOUND, 2)))\n",
        "        parser.add_argument(\"-m\", \"--model\", help=\"Model file to load\")\n",
        "        args = parser.parse_args()\n",
        "\n",
        "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "    # Make Gym environement and DQNs\n",
        "    if COLAB:\n",
        "        env = make_env(args.env)\n",
        "        net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "        target_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "    else:\n",
        "        env = wrappers.make_env(args.env)\n",
        "        net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "        target_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "        writer = SummaryWriter(comment=\"-\" + args.env)\n",
        "\n",
        "    print(net)\n",
        "\n",
        "    replay_memory = ExperienceReplay(REPLAY_SIZE)\n",
        "    agent = Agent(env, replay_memory)\n",
        "    epsilon = EPSILON_START\n",
        "\n",
        "    if LOAD_MODEL:\n",
        "        net.load_state_dict(torch.load(args.model, map_location=lambda storage, loc: storage))\n",
        "        target_net.load_state_dict(net.state_dict())\n",
        "        print(\"Models loaded from disk!\")\n",
        "        # Lower exploration rate\n",
        "        EPSILON_START = EPSILON_FINAL\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "    total_rewards = []\n",
        "    best_mean_reward = None\n",
        "    frame_idx = 0\n",
        "    timestep_frame = 0\n",
        "    timestep = time.time()\n",
        "\n",
        "    while True:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY)\n",
        "\n",
        "        reward = agent.play_step(net, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            total_rewards.append(reward)\n",
        "            speed = (frame_idx - timestep_frame) / (time.time() - timestep)\n",
        "            timestep_frame = frame_idx\n",
        "            timestep = time.time()\n",
        "            mean_reward = np.mean(total_rewards[-100:])\n",
        "            print(\"{} frames: done {} games, mean reward {}, eps {}, speed {} f/s\".format(\n",
        "                frame_idx, len(total_rewards), round(mean_reward, 3), round(epsilon,2), round(speed, 2)))\n",
        "            if not COLAB:\n",
        "                writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "                writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "                writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
        "                writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward or len(total_rewards) % 25 == 0:\n",
        "                torch.save(net.state_dict(), args.env + \"-\" + str(len(total_rewards)) + \".dat\")\n",
        "                if COLAB:\n",
        "                    gsync.update_file_to_folder(args.env + \"-\" + str(len(total_rewards)) + \".dat\")\n",
        "                if best_mean_reward is not None:\n",
        "                    print(\"New best mean reward {} -> {}, model saved\".format(round(best_mean_reward, 3), round(mean_reward, 3)))\n",
        "                best_mean_reward = mean_reward\n",
        "            if mean_reward > args.reward and len(total_rewards) > 10:\n",
        "                print(\"Game solved in {} frames! Average score of {}\".format(frame_idx, mean_reward))\n",
        "                break\n",
        "\n",
        "        if len(replay_memory) < LEARNING_STARTS:\n",
        "            continue\n",
        "\n",
        "        if frame_idx % TARGET_UPDATE_FREQ == 0:\n",
        "            target_net.load_state_dict(net.state_dict())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch = replay_memory.sample(BATCH_SIZE)\n",
        "        loss_t = calculate_loss(batch, net, target_net, device=device)\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "    env.close()\n",
        "    if not COLAB:\n",
        "        writer.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}